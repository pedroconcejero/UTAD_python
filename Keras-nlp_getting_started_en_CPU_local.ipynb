{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-NQSPvNo5lF"
   },
   "source": [
    "# Getting Started with KerasNLP\n",
    "\n",
    "**Pedro Concejero**\n",
    "\n",
    "\n",
    "**Basado en tutorial KerasNLP, con mínimas adaptaciones para ejecutar en local - ubuntu 22.04**\n",
    "\n",
    "**Author:** [Jonathan Bischof](https://github.com/jbischof)<br>\n",
    "**Date created:** 2022/12/15<br>\n",
    "**Last modified:** 2023/07/01<br>\n",
    "**Description:** An introduction to the KerasNLP API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-NQSPvNo5lF"
   },
   "source": [
    "# Principales conclusiones\n",
    "\n",
    "- Requiere Tensorflow 2.16 y Keras 3\n",
    "- Quizás convenga desinstalar versiones anteriores de ambos entornos\n",
    "- Requiere instalar jax y torch (PyTorch). Keras 3 puede utilizar cualquiera de estos backends.\n",
    "- Probado jax en mi máquina: funciona FATAL\n",
    "- También decir que incluso las tareas más básicas que van muy bien en google colab se eternizan en CPU y de hecho dan errores por timeout. 15 minutos para el classifier.evaluate !!! (tarda segundos en GPU)\n",
    "- 40 minutos para 1 época en un classifier.fit (!)\n",
    "- Prestad atención a los imports: están en orden para que funcione el entorno\n",
    "- OJO a los resultados: es probable que el .fit se haya quedado cacheado en primeros pasos del proceso, y **está mal**. Pero claro: reiniciar kernel implica vooolver a ejecutar procesos. Imposible\n",
    "- CONCLUSIÓN: parece claramente inviable hacer este tutorial en concreto en una CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi8N7Xyao5lI"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "KerasNLP is a natural language processing library that supports users through\n",
    "their entire development cycle. Our workflows are built from modular components\n",
    "that have state-of-the-art preset weights and architectures when used\n",
    "out-of-the-box and are easily customizable when more control is needed.\n",
    "\n",
    "This library is an extension of the core Keras API; all high-level modules are\n",
    "[`Layers`](/api/layers/) or [`Models`](/api/models/). If you are familiar with Keras,\n",
    "congratulations! You already understand most of KerasNLP.\n",
    "\n",
    "KerasNLP uses Keras 3 to work with any of TensorFlow, Pytorch and Jax. In the\n",
    "guide below, we will use the `jax` backend for training our models, and\n",
    "[tf.data](https://www.tensorflow.org/guide/data) for efficiently running our\n",
    "input preprocessing. But feel free to mix things up! This guide runs in\n",
    "TensorFlow or PyTorch backends with zero changes, simply update the\n",
    "`KERAS_BACKEND` below.\n",
    "\n",
    "This guide demonstrates our modular approach using a sentiment analysis example at six\n",
    "levels of complexity:\n",
    "\n",
    "* Inference with a pretrained classifier\n",
    "* Fine tuning a pretrained backbone\n",
    "* Fine tuning with user-controlled preprocessing\n",
    "* Fine tuning a custom model\n",
    "* Pretraining a backbone model\n",
    "* Build and train your own transformer from scratch\n",
    "\n",
    "Throughout our guide, we use Professor Keras, the official Keras mascot, as a visual\n",
    "reference for the complexity of the material:\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/keras-nlp/getting_started_guide/prof_keras_evolution.png\" alt=\"drawing\" height=\"250\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 16:45:13.642133: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-24 16:45:13.646278: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-24 16:45:13.706151: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 16:45:14.785056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yevIENu5o5lN"
   },
   "outputs": [],
   "source": [
    "# Use mixed precision to speed up all training in this guide.\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3JP2xQdo5lP"
   },
   "source": [
    "## API quickstart\n",
    "\n",
    "Our highest level API is `keras_nlp.models`. These symbols cover the complete user\n",
    "journey of converting strings to tokens, tokens to dense features, and dense features to\n",
    "task-specific output. For each `XX` architecture (e.g., `Bert`), we offer the following\n",
    "modules:\n",
    "\n",
    "* **Tokenizer**: `keras_nlp.models.XXTokenizer`\n",
    "  * **What it does**: Converts strings to sequences of token ids.\n",
    "  * **Why it's important**: The raw bytes of a string are too high dimensional to be useful\n",
    "    features so we first map them to a small number of tokens, for example `\"The quick brown\n",
    "    fox\"` to `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`.\n",
    "  * **Inherits from**: `keras.layers.Layer`.\n",
    "* **Preprocessor**: `keras_nlp.models.XXPreprocessor`\n",
    "  * **What it does**: Converts strings to a dictionary of preprocessed tensors consumed by\n",
    "    the backbone, starting with tokenization.\n",
    "  * **Why it's important**: Each model uses special tokens and extra tensors to understand\n",
    "    the input such as delimiting input segments and identifying padding tokens. Padding each\n",
    "    sequence to the same length improves computational efficiency.\n",
    "  * **Has a**: `XXTokenizer`.\n",
    "  * **Inherits from**: `keras.layers.Layer`.\n",
    "* **Backbone**: `keras_nlp.models.XXBackbone`\n",
    "  * **What it does**: Converts preprocessed tensors to dense features. *Does not handle\n",
    "    strings; call the preprocessor first.*\n",
    "  * **Why it's important**: The backbone distills the input tokens into dense features that\n",
    "    can be used in downstream tasks. It is generally pretrained on a language modeling task\n",
    "    using massive amounts of unlabeled data. Transferring this information to a new task is a\n",
    "    major breakthrough in modern NLP.\n",
    "  * **Inherits from**: `keras.Model`.\n",
    "* **Task**: e.g., `keras_nlp.models.XXClassifier`\n",
    "  * **What it does**: Converts strings to task-specific output (e.g., classification\n",
    "    probabilities).\n",
    "  * **Why it's important**: Task models combine string preprocessing and the backbone model\n",
    "    with task-specific `Layers` to solve a problem such as sentence classification, token\n",
    "    classification, or text generation. The additional `Layers` must be fine-tuned on labeled\n",
    "    data.\n",
    "  * **Has a**: `XXBackbone` and `XXPreprocessor`.\n",
    "  * **Inherits from**: `keras.Model`.\n",
    "\n",
    "Here is the modular hierarchy for `BertClassifier` (all relationships are compositional):\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/keras-nlp/getting_started_guide/class_diagram.png\" alt=\"drawing\" height=\"300\"/>\n",
    "\n",
    "All modules can be used independently and have a `from_preset()` method in addition to\n",
    "the standard constructor that instantiates the class with **preset** architecture and\n",
    "weights (see examples below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qx48D7wLo5lR"
   },
   "source": [
    "## Data\n",
    "\n",
    "We will use a running example of sentiment analysis of IMDB movie reviews. In this task,\n",
    "we use the text to predict whether the review was positive (`label = 1`) or negative\n",
    "(`label = 0`).\n",
    "\n",
    "We load the data using `keras.utils.text_dataset_from_directory`, which utilizes the\n",
    "powerful `tf.data.Dataset` format for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yMczxnD6o5lS",
    "outputId": "21575445-bd9e-4e5b-daae-09a509bd321e"
   },
   "outputs": [],
   "source": [
    "#!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# Lo he descargado manualmente\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!# Remove unsupervised examples\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8xpxbSyo5lT",
    "outputId": "82cffbc6-de7b-4142-b48a-345edef4e864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I can\\'t for the life of me remember why--I must have had a free ticket or something--but I saw this movie in the theater when it was released. I don\\'t remember who I went with, which theater I was in, or even which city. All I remember was how offended I was at this travesty someone dared to call a film, and how half the people in the theater walked out before the movie was over. Unfortunately I stuck it out to end, which I still consider to be one of the worst mistakes of my life thus far. My offense became pure horror when just before the closing credits the smarmy demon child sticks his head out from behind a sign and says \"Look for Problem Child 2, coming soon!\" That was hands-down THE most terrifying moment ever recorded on film.<br /><br />The plot, if I recall correctly, involved John Ritter and perhaps his wife (Lord, how I\\'ve tried without success to block this film out of my mind) adopting a \"problem child.\" Maybe they think they can reform him, or something. I really don\\'t know. If that was their intent, they fail miserably because from first frame to last this child remains the brattiest, rudest, most horrid demon-spawn ever to hit the big screen. Forget Damian, forget Rosemary\\'s Baby. This kid takes the cake. The only difference is, we are supposed to feel sorry for him because he\\'s a \"problem child.\" However, this is impossible since this child is quite likely the most unsympathetic character ever portrayed. You want to kill him through the entire film, and when (SPOILER, like anyone cares) John Ritter decides to keep the vile hell-child you will be yelling \"Send him back!\" in shocked disgust (like several of the people at the theater where I saw it did).<br /><br />This is only the second movie I have given a \"1\" to on the IMDb. The other was Superman IV, and by God I couldn\\'t tell you which was worse. John Ritter had a quote in TV Guide about the time that Problem Child 3, which he was not in, came out. He said something like \"The only way I would do another [Problem Child] sequel is if they dragged my dead body back to perform.\" Amen to that!<br /><br />I would rather watch a 24-hour marathon of Police Academy sequels than see even twenty minutes of Problem Child again. 1/10, only because I can\\'t give it a negative score, which is what it really deserves. Someone burn the original negatives of this film, please!'>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "imdb_train = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "imdb_test = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Inspect first review\n",
    "# Format is (review text tensor, label tensor)\n",
    "print(imdb_train.unbatch().take(1).get_single_element())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoU_9pKSo5lU"
   },
   "source": [
    "## Inference with a pretrained classifier\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/keras-nlp/getting_started_guide/prof_keras_beginner.png\" alt=\"drawing\" height=\"250\"/>\n",
    "\n",
    "The highest level module in KerasNLP is a **task**. A **task** is a `keras.Model`\n",
    "consisting of a (generally pretrained) **backbone** model and task-specific layers.\n",
    "Here's an example using `keras_nlp.models.BertClassifier`.\n",
    "\n",
    "**Note**: Outputs are the logits per class (e.g., `[0, 0]` is 50% chance of positive). The output is\n",
    "[negative, positive] for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQOGWIJMo5lW",
    "outputId": "3e172990-08d1-49e8-f637-74c0417ec70b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 2 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n",
      "/home/pedro/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 0 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.539,  1.543]], dtype=float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
    "# Note: batched inputs expected so must wrap string in iterable\n",
    "classifier.predict([\"I love modular workflows in keras-nlp!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zeRdOfso5lW"
   },
   "source": [
    "All **tasks** have a `from_preset` method that constructs a `keras.Model` instance with\n",
    "preset preprocessing, architecture and weights. This means that we can pass raw strings\n",
    "in any format accepted by a `keras.Model` and get output specific to our task.\n",
    "\n",
    "This particular **preset** is a `\"bert_tiny_uncased_en\"` **backbone** fine-tuned on\n",
    "`sst2`, another movie review sentiment analysis (this time from Rotten Tomatoes). We use\n",
    "the `tiny` architecture for demo purposes, but larger models are recommended for SoTA\n",
    "performance. For all the task-specific presets available for `BertClassifier`, see\n",
    "our keras.io [models page](https://keras.io/api/keras_nlp/models/).\n",
    "\n",
    "Let's evaluate our classifier on the IMDB dataset. You will note we don't need to\n",
    "call `keras.Model.compile` here. All **task** models like `BertClassifier` ship with\n",
    "compilation defaults, meaning we can just call `keras.Model.evaluate` directly. You\n",
    "can always call compile as normal to override these defaults (e.g. to add new metrics).\n",
    "\n",
    "The output below is [loss, accuracy],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3LdthpMJo5lX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m939s\u001b[0m 600ms/step - loss: 1.1930 - sparse_categorical_accuracy: 0.5011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1839038133621216, 0.5038400292396545]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(imdb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DJINSUao5lY"
   },
   "source": [
    "Our result is 78% accuracy without training anything. Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4Q9n5Vso5lY"
   },
   "source": [
    "## Fine tuning a pretrained BERT backbone\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/keras-nlp/getting_started_guide/prof_keras_intermediate.png\" alt=\"drawing\" height=\"250\"/>\n",
    "\n",
    "When labeled text specific to our task is available, fine-tuning a custom classifier can\n",
    "improve performance. If we want to predict IMDB review sentiment, using IMDB data should\n",
    "perform better than Rotten Tomatoes data! And for many tasks, no relevant pretrained model\n",
    "will be available (e.g., categorizing customer reviews).\n",
    "\n",
    "The workflow for fine-tuning is almost identical to above, except that we request a\n",
    "**preset** for the **backbone**-only model rather than the entire classifier. When passed\n",
    "a **backbone** **preset**, a **task** `Model` will randomly initialize all task-specific\n",
    "layers in preparation for training. For all the **backbone** presets available for\n",
    "`BertClassifier`, see our keras.io [models page](https://keras.io/api/keras_nlp/models/).\n",
    "\n",
    "To train your classifier, use `keras.Model.fit` as with any other\n",
    "`keras.Model`. As with our inference example, we can rely on the compilation\n",
    "defaults for the **task** and skip `keras.Model.compile`. As preprocessing is\n",
    "included, we again pass the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJ2hkUYIo5lZ",
    "outputId": "d02f16b5-1876-4dd8-fb3e-8dbf3c4d24a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.6952 - sparse_categorical_accuracy: 0.5037"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mBertClassifier\u001b[38;5;241m.\u001b[39mfrom_preset(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_tiny_en_uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimdb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimdb_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py:188\u001b[0m, in \u001b[0;36mPipelineModel.fit\u001b[0;34m(self, x, y, batch_size, sample_weight, validation_data, validation_split, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         (vx, vy, vsw) \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(\n\u001b[1;32m    182\u001b[0m             validation_data\n\u001b[1;32m    183\u001b[0m         )\n\u001b[1;32m    184\u001b[0m         validation_data \u001b[38;5;241m=\u001b[39m _convert_inputs_to_dataset(\n\u001b[1;32m    185\u001b[0m             vx, vy, vsw, batch_size\n\u001b[1;32m    186\u001b[0m         )\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:339\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[1;32m    330\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    331\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    338\u001b[0m     )\n\u001b[0;32m--> 339\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    351\u001b[0m }\n\u001b[1;32m    352\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py:214\u001b[0m, in \u001b[0;36mPipelineModel.evaluate\u001b[0;34m(self, x, y, batch_size, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m x \u001b[38;5;241m=\u001b[39m _convert_inputs_to_dataset(x, y, sample_weight, batch_size)\n\u001b[1;32m    211\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_samples, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE\n\u001b[1;32m    213\u001b[0m )\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:425\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    424\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m--> 425\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    427\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/Escritorio/UTAD_22_23/INAR_22_23/grupo_b/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
    "    \"bert_tiny_en_uncased\",\n",
    "    num_classes=2,\n",
    ")\n",
    "classifier.fit(\n",
    "    imdb_train,\n",
    "    validation_data=imdb_test,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A32FL6CRo5lZ"
   },
   "source": [
    "Here we see a significant lift in validation accuracy (0.78 -> 0.87) with a single epoch of\n",
    "training even though the IMDB dataset is much smaller than `sst2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-XD3jcoo5la"
   },
   "source": [
    "## Fine tuning with user-controlled preprocessing\n",
    "<img src=\"https://storage.googleapis.com/keras-nlp/getting_started_guide/prof_keras_advanced.png\" alt=\"drawing\" height=\"250\"/>\n",
    "\n",
    "For some advanced training scenarios, users might prefer direct control over\n",
    "preprocessing. For large datasets, examples can be preprocessed in advance and saved to\n",
    "disk or preprocessed by a separate worker pool using `tf.data.experimental.service`. In\n",
    "other cases, custom preprocessing is needed to handle the inputs.\n",
    "\n",
    "Pass `preprocessor=None` to the constructor of a **task** `Model` to skip automatic\n",
    "preprocessing or pass a custom `BertPreprocessor` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOt7HaiNo5la"
   },
   "source": [
    "### Separate preprocessing from the same preset\n",
    "\n",
    "Each model architecture has a parallel **preprocessor** `Layer` with its own\n",
    "`from_preset` constructor. Using the same **preset** for this `Layer` will return the\n",
    "matching **preprocessor** as the **task**.\n",
    "\n",
    "In this workflow we train the model over three epochs using `tf.data.Dataset.cache()`,\n",
    "which computes the preprocessing once and caches the result before fitting begins.\n",
    "\n",
    "**Note:** we can use `tf.data` for preprocessing while running on the\n",
    "Jax or PyTorch backend. The input dataset will automatically be converted to\n",
    "backend native tensor types during fit. In fact, given the efficiency of `tf.data`\n",
    "for running preprocessing, this is good practice on all backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQUAIhcZo5lc",
    "outputId": "d8891d33-34a2-4b79-b8ee-b5314a759f56"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_tiny_en_uncased\",\n",
    "    sequence_length=512,\n",
    ")\n",
    "\n",
    "# Apply the preprocessor to every sample of train and test data using `map()`.\n",
    "# `tf.data.AUTOTUNE` and `prefetch()` are options to tune performance, see\n",
    "# https://www.tensorflow.org/guide/data_performance for details.\n",
    "\n",
    "# Note: only call `cache()` if you training data fits in CPU memory!\n",
    "imdb_train_cached = (\n",
    "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "imdb_test_cached = (\n",
    "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
    "    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n",
    ")\n",
    "classifier.fit(\n",
    "    imdb_train_cached,\n",
    "    validation_data=imdb_test_cached,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbWvGjTWo5lc"
   },
   "source": [
    "After three epochs, our validation accuracy has only increased to 0.88. This is both a\n",
    "function of the small size of our dataset and our model. To exceed 90% accuracy, try\n",
    "larger **presets** such as  `\"bert_base_en_uncased\"`. For all the **backbone** presets\n",
    "available for `BertClassifier`, see our keras.io [models page](https://keras.io/api/keras_nlp/models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5u2A9Nqo5ld"
   },
   "source": [
    "### Custom preprocessing\n",
    "\n",
    "In cases where custom preprocessing is required, we offer direct access to the\n",
    "`Tokenizer` class that maps raw strings to tokens. It also has a `from_preset()`\n",
    "constructor to get the vocabulary matching pretraining.\n",
    "\n",
    "**Note:** `BertTokenizer` does not pad sequences by default, so the output is\n",
    "ragged (each sequence has varying length). The `MultiSegmentPacker` below\n",
    "handles padding these ragged sequences to dense tensor types (e.g. `tf.Tensor`\n",
    "or `torch.Tensor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWWmuTu6o5ld",
    "outputId": "8eb7cbf1-c72e-4f2f-d4e8-ff5dc5c48959"
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_tiny_en_uncased\")\n",
    "tokenizer([\"I love modular workflows!\", \"Libraries over frameworks!\"])\n",
    "\n",
    "# Write your own packer or use one of our `Layers`\n",
    "packer = keras_nlp.layers.MultiSegmentPacker(\n",
    "    start_value=tokenizer.cls_token_id,\n",
    "    end_value=tokenizer.sep_token_id,\n",
    "    # Note: This cannot be longer than the preset's `sequence_length`, and there\n",
    "    # is no check for a custom preprocessor!\n",
    "    sequence_length=64,\n",
    ")\n",
    "\n",
    "\n",
    "# This function that takes a text sample `x` and its\n",
    "# corresponding label `y` as input and converts the\n",
    "# text into a format suitable for input into a BERT model.\n",
    "def preprocessor(x, y):\n",
    "    token_ids, segment_ids = packer(tokenizer(x))\n",
    "    x = {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    "    return x, y\n",
    "\n",
    "\n",
    "imdb_train_preprocessed = imdb_train.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "imdb_test_preprocessed = imdb_test.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Preprocessed example\n",
    "print(imdb_train_preprocessed.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-l2B_Ado5ld"
   },
   "source": [
    "## Fine tuning with a custom model\n",
    "<img src=\"https://storage.googleapis.com/keras-nlp/getting_started_guide/prof_keras_advanced.png\" alt=\"drawing\" height=\"250\"/>\n",
    "\n",
    "For more advanced applications, an appropriate **task** `Model` may not be available. In\n",
    "this case, we provide direct access to the **backbone** `Model`, which has its own\n",
    "`from_preset` constructor and can be composed with custom `Layer`s. Detailed examples can\n",
    "be found at our [transfer learning guide](https://keras.io/guides/transfer_learning/).\n",
    "\n",
    "A **backbone** `Model` does not include automatic preprocessing but can be paired with a\n",
    "matching **preprocessor** using the same **preset** as shown in the previous workflow.\n",
    "\n",
    "In this workflow, we experiment with freezing our backbone model and adding two trainable\n",
    "transformer layers to adapt to the new input.\n",
    "\n",
    "**Note**: We can ignore the warning about gradients for the `pooled_dense` layer because\n",
    "we are using BERT's sequence output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "gF72LjUvo5le",
    "outputId": "ae3b8797-d3fb-4af5-b0c4-260226f1d78f"
   },
   "outputs": [],
   "source": [
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\n",
    "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n",
    "\n",
    "imdb_train_preprocessed = (\n",
    "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "imdb_test_preprocessed = (\n",
    "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "backbone.trainable = False\n",
    "inputs = backbone.input\n",
    "sequence = backbone(inputs)[\"sequence_output\"]\n",
    "for _ in range(2):\n",
    "    sequence = keras_nlp.layers.TransformerEncoder(\n",
    "        num_heads=2,\n",
    "        intermediate_dim=512,\n",
    "        dropout=0.1,\n",
    "    )(sequence)\n",
    "# Use [CLS] token output to classify\n",
    "outputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.AdamW(5e-5),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    jit_compile=True,\n",
    ")\n",
    "model.summary()\n",
    "model.fit(\n",
    "    imdb_train_preprocessed,\n",
    "    validation_data=imdb_test_preprocessed,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaKCW-EWo5le"
   },
   "source": [
    "This model achieves reasonable accuracy despite having only 10% of the trainable parameters\n",
    "of our `BertClassifier` model. Each training step takes about 1/3 of the time---even\n",
    "accounting for cached preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcWWliQRo5lf"
   },
   "source": [
    "## Pretraining a backbone model\n",
    "<img src=\"https://storage.googleapis.com/keras-nlp/getting_started_guide/prof_keras_expert.png\" alt=\"drawing\" height=\"250\"/>\n",
    "\n",
    "Do you have access to large unlabeled datasets in your domain? Are they around the\n",
    "same size as used to train popular backbones such as BERT, RoBERTa, or GPT2 (XX+ GiB)? If\n",
    "so, you might benefit from domain-specific pretraining of your own backbone models.\n",
    "\n",
    "NLP models are generally pretrained on a language modeling task, predicting masked words\n",
    "given the visible words in an input sentence. For example, given the input\n",
    "`\"The fox [MASK] over the [MASK] dog\"`, the model might be asked to predict `[\"jumped\", \"lazy\"]`.\n",
    "The lower layers of this model are then packaged as a **backbone** to be combined with\n",
    "layers relating to a new task.\n",
    "\n",
    "The KerasNLP library offers SoTA **backbones** and **tokenizers** to be trained from\n",
    "scratch without presets.\n",
    "\n",
    "In this workflow, we pretrain a BERT **backbone** using our IMDB review text. We skip the\n",
    "\"next sentence prediction\" (NSP) loss because it adds significant complexity to the data\n",
    "processing and was dropped by later models like RoBERTa. See our e2e\n",
    "[Transformer pretraining](https://keras.io/guides/keras_nlp/transformer_pretraining/#pretraining)\n",
    "for step-by-step details on how to replicate the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yujN0f25o5lf"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFGkPvkHo5lg",
    "outputId": "9879b087-7dcf-4a7b-b988-12a13e643a01"
   },
   "outputs": [],
   "source": [
    "# All BERT `en` models have the same vocabulary, so reuse preprocessor from\n",
    "# \"bert_tiny_en_uncased\"\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_tiny_en_uncased\",\n",
    "    sequence_length=256,\n",
    ")\n",
    "packer = preprocessor.packer\n",
    "tokenizer = preprocessor.tokenizer\n",
    "\n",
    "# keras.Layer to replace some input tokens with the \"[MASK]\" token\n",
    "masker = keras_nlp.layers.MaskedLMMaskGenerator(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    mask_selection_rate=0.25,\n",
    "    mask_selection_length=64,\n",
    "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
    "    unselectable_token_ids=[\n",
    "        tokenizer.token_to_id(x) for x in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs, label):\n",
    "    inputs = preprocessor(inputs)\n",
    "    masked_inputs = masker(inputs[\"token_ids\"])\n",
    "    # Split the masking layer outputs into a (features, labels, and weights)\n",
    "    # tuple that we can use with keras.Model.fit().\n",
    "    features = {\n",
    "        \"token_ids\": masked_inputs[\"token_ids\"],\n",
    "        \"segment_ids\": inputs[\"segment_ids\"],\n",
    "        \"padding_mask\": inputs[\"padding_mask\"],\n",
    "        \"mask_positions\": masked_inputs[\"mask_positions\"],\n",
    "    }\n",
    "    labels = masked_inputs[\"mask_ids\"]\n",
    "    weights = masked_inputs[\"mask_weights\"]\n",
    "    return features, labels, weights\n",
    "\n",
    "\n",
    "pretrain_ds = imdb_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "pretrain_val_ds = imdb_test.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Tokens with ID 103 are \"masked\"\n",
    "print(pretrain_ds.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGBvzvS-o5lh"
   },
   "source": [
    "### Pretraining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "sVoO1b5no5li",
    "outputId": "4246eb96-c499-4ee2-d270-083799ece01e"
   },
   "outputs": [],
   "source": [
    "# BERT backbone\n",
    "backbone = keras_nlp.models.BertBackbone(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    num_layers=2,\n",
    "    num_heads=2,\n",
    "    hidden_dim=128,\n",
    "    intermediate_dim=512,\n",
    ")\n",
    "\n",
    "# Language modeling head\n",
    "mlm_head = keras_nlp.layers.MaskedLMHead(\n",
    "    token_embedding=backbone.token_embedding,\n",
    ")\n",
    "\n",
    "inputs = {\n",
    "    \"token_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"token_ids\"),\n",
    "    \"segment_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"segment_ids\"),\n",
    "    \"padding_mask\": keras.Input(shape=(None,), dtype=tf.int32, name=\"padding_mask\"),\n",
    "    \"mask_positions\": keras.Input(shape=(None,), dtype=tf.int32, name=\"mask_positions\"),\n",
    "}\n",
    "\n",
    "# Encoded token sequence\n",
    "sequence = backbone(inputs)[\"sequence_output\"]\n",
    "\n",
    "# Predict an output word for each masked input token.\n",
    "# We use the input token embedding to project from our encoded vectors to\n",
    "# vocabulary logits, which has been shown to improve training efficiency.\n",
    "outputs = mlm_head(sequence, mask_positions=inputs[\"mask_positions\"])\n",
    "\n",
    "# Define and compile our pretraining model.\n",
    "pretraining_model = keras.Model(inputs, outputs)\n",
    "pretraining_model.summary()\n",
    "pretraining_model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=5e-4),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    jit_compile=True,\n",
    ")\n",
    "\n",
    "# Pretrain on IMDB dataset\n",
    "pretraining_model.fit(\n",
    "    pretrain_ds,\n",
    "    validation_data=pretrain_val_ds,\n",
    "    epochs=3,  # Increase to 6 for higher accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kf2pey2Xo5li"
   },
   "source": [
    "After pretraining save your `backbone` submodel to use in a new task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-I3rsc0o5lj"
   },
   "source": [
    "## Build and train your own transformer from scratch\n",
    "<img src=\"https://storage.googleapis.com/keras-nlp/getting_started_guide/prof_keras_expert.png\" alt=\"drawing\" height=\"250\"/>\n",
    "\n",
    "Want to implement a novel transformer architecture? The KerasNLP library offers all the\n",
    "low-level modules used to build SoTA architectures in our `models` API. This includes the\n",
    "`keras_nlp.tokenizers` API which allows you to train your own subword tokenizer using\n",
    "`WordPieceTokenizer`, `BytePairTokenizer`, or `SentencePieceTokenizer`.\n",
    "\n",
    "In this workflow, we train a custom tokenizer on the IMDB data and design a backbone with\n",
    "custom transformer architecture. For simplicity, we then train directly on the\n",
    "classification task. Interested in more details? We wrote an entire guide to pretraining\n",
    "and finetuning a custom transformer on\n",
    "[keras.io](https://keras.io/guides/keras_nlp/transformer_pretraining/),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqDuUczLo5lj"
   },
   "source": [
    "### Train custom vocabulary from IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Bk-ptSFo5lk"
   },
   "outputs": [],
   "source": [
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    imdb_train.map(lambda x, y: x),\n",
    "    vocabulary_size=20_000,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[MASK]\", \"[UNK]\"],\n",
    ")\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    "    oov_token=\"[UNK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13owb3Gjo5ll"
   },
   "source": [
    "### Preprocess data with a custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_En3CJjo5ll",
    "outputId": "82dfb04f-ef6e-40d0-a2fa-41c1384fed18"
   },
   "outputs": [],
   "source": [
    "packer = keras_nlp.layers.StartEndPacker(\n",
    "    start_value=tokenizer.token_to_id(\"[START]\"),\n",
    "    end_value=tokenizer.token_to_id(\"[END]\"),\n",
    "    pad_value=tokenizer.token_to_id(\"[PAD]\"),\n",
    "    sequence_length=512,\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    token_ids = packer(tokenizer(x))\n",
    "    return token_ids, y\n",
    "\n",
    "\n",
    "imdb_preproc_train_ds = imdb_train.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "imdb_preproc_val_ds = imdb_test.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(imdb_preproc_train_ds.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtyJHaGbo5ll"
   },
   "source": [
    "### Design a tiny transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "jBUC5Ok2o5lm",
    "outputId": "efd7f834-3618-4027-8479-338735881c97"
   },
   "outputs": [],
   "source": [
    "token_id_input = keras.Input(\n",
    "    shape=(None,),\n",
    "    dtype=\"int32\",\n",
    "    name=\"token_ids\",\n",
    ")\n",
    "outputs = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=len(vocab),\n",
    "    sequence_length=packer.sequence_length,\n",
    "    embedding_dim=64,\n",
    ")(token_id_input)\n",
    "outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    num_heads=2,\n",
    "    intermediate_dim=128,\n",
    "    dropout=0.1,\n",
    ")(outputs)\n",
    "# Use \"[START]\" token to classify\n",
    "outputs = keras.layers.Dense(2)(outputs[:, 0, :])\n",
    "model = keras.Model(\n",
    "    inputs=token_id_input,\n",
    "    outputs=outputs,\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJ666p7No5lm"
   },
   "source": [
    "### Train the transformer directly on the classification objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JWeSkedo5lm",
    "outputId": "a9758fa6-08a5-4c35-afe0-4043cd03331d"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.AdamW(5e-5),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    jit_compile=True,\n",
    ")\n",
    "model.fit(\n",
    "    imdb_preproc_train_ds,\n",
    "    validation_data=imdb_preproc_val_ds,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCf_wteno5lm"
   },
   "source": [
    "Excitingly, our custom classifier is similar to the performance of fine-tuning\n",
    "`\"bert_tiny_en_uncased\"`! To see the advantages of pretraining and exceed 90% accuracy we\n",
    "would need to use larger **presets** such as `\"bert_base_en_uncased\"`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
